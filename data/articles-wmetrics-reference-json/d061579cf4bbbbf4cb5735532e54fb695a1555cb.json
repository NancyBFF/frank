{"bart_reference": "Obama promised Armenian-Americans he would call the atrocity genocide during the 2008 campaign . The White House views Turkey as a more crucial ally than Armenia . Pope Francis , actor George Clooney , and even the Kardashians have taken the moral position , calling it the Armenian genocide .", "bart": "on the 2008 campaign trail , obama promised to use the word `` genocide '' to describe the 1915 massacre by turks of armenians . but that was then . now , as was the case with bush , obama regards turkey as a more crucial ally than armenia . turkey denies this history .", "id": "cnn-test-d061579cf4bbbbf4cb5735532e54fb695a1555cb", "filepath": "cnndm/cnn/stories/d061579cf4bbbbf4cb5735532e54fb695a1555cb.story", "bert_sum_reference": "obama promised armenian-americans he would call the atrocity genocide during the 2008 campaign . . the white house views turkey as a more crucial ally than armenia . . pope francis , actor george clooney , and even the kardashians have taken the moral position , calling it the armenian genocide . .", "bert_sum": "this week is the 100th anniversary of what many historians acknowledge as the armenian genocide . president barack obama has broken his promise to use the word \" genocide \" to describe the atrocity . it 's a moral position taken by pope francis , george clooney and even by the kardashians .", "bus_reference": "obama promised armenian-americans he would call the atrocity genocide during the 2008 campaign . the white house views turkey as a more crucial ally than armenia . pope francis , actor george clooney , and even the kardashians have taken the moral position , calling it the armenian genocide .", "bus": "obama promised to use the word `` genocide '' by turks of armenians . it 's a moral year in a president obama 's promise to call it a genocide . turkey has a military in nato .", "pgn_reference": "obama promised armenian-americans he would call the atrocity genocide during the 2008 campaign . the white house views turkey as a more crucial ally than armenia . pope francis , actor george clooney , and even the kardashians have taken the moral position , calling it the armenian genocide .", "pgn": "obama promised to use the word `` genocide '' to describe the 1915 massacre by turks of armenians . it 's a moral position taken by pope francis , actor george clooney and even by the kardashians .", "s2s_reference": "obama promised armenian-americans he would call the atrocity genocide during the 2008 campaign . the white house views turkey as a more crucial ally than armenia . pope francis , actor george clooney , and even the kardashians have taken the moral position , calling it the armenian genocide .", "s2s": "the genocide took place in the armenian genocide of an estimated 1.5 million armenians . the genocide took place in the armenian genocide of an estimated 1.5 million armenians . the genocide took place in the armenian genocide of an estimated 1.5 million armenians .", "hash": "d061579cf4bbbbf4cb5735532e54fb695a1555cb", "url": "http://web.archive.org/web/20150426055312id_/http://edition.cnn.com/2015/04/24/politics/armenia-genocide-obama-broken-promise-jake-tapper/index.html", "article": "Washington (CNN)This week is the 100th anniversary of what many historians acknowledge as the Armenian genocide -- the Turkish massacre of an estimated 1.5 million Armenians And it's also the seventh year in a row President Barack Obama has broken his promise to use the word \"genocide\" to describe the atrocity. It's a moral position taken by Pope Francis, actor George Clooney and even by the Kardashians. On the 2008 campaign trail, Obama promised to use the word \"genocide\" to describe the 1915 massacre by Turks of Armenians -- a pledge he made when seeking Armenian-American votes. Back then, he held up his willingness to call it a \"genocide\" as an example of why he was the kind of truth-telling candidate the nation needed. 8 things to know about the mass killings of Armenians 100 years ago In 2006, after the U.S. Ambassador to Armenia was asked to resign for using the term Armenian genocide, then-Sen. Obama hammered the Bush administration for not taking a stand. \"The Armenian genocide is not an allegation, a personal opinion, or a point of view, but rather a widely documented fact supported by an overwhelming body of historical evidence,\" he said. But that was then. And now, as was the case with Bush, Obama regards Turkey -- the only Muslim majority country in NATO -- as a more crucial ally than Armenia. Turkey has the second-largest military in NATO, behind only the U.S., and is a crucial ally when it comes to Syria, ISIS, Iran and other Middle East issues. And Turkey denies this history. \"We cannot define what happened in 1915 as a genocide,\" Turkish Foreign Minister Mevlut Cavusoglu told CNN on Tuesday. Why Turkey won't say the G-word when it comes to the Armenians In her Pulitzer Prize-winning book about genocide, Obama's current Ambassador to the United Nations Samantha Power hammered U.S. policy makers for not acknowledging or acting to stop such atrocities. \"No U.S. president has ever made genocide prevention a priority, and no U.S. president has ever suffered politically for his indifference to its occurrence. It is thus no coincidence that genocide rages on,\" she wrote.", "entity_counter": {"the 100th anniversary": 1, "many historians": 1, "the Armenian genocide": 1, "the Turkish massacre": 1, "an estimated 1.5 million Armenians": 1, "President Barack Obama": 3, "his promise": 1, "the word \"genocide": 1, "the atrocity": 1, "a moral position": 1, "Pope Francis": 1, "actor George Clooney": 1, "the Kardashians": 1, "the 2008 campaign trail": 1, "the word": 1, "\"genocide": 1, "the 1915 massacre": 1, "Turks": 1, "Armenians": 2, "a pledge": 1, "Armenian-American votes": 1, "his willingness": 1, "an example": 1, "the kind": 1, "truth-telling candidate": 1, "the nation": 1, "8 things": 1, "the mass killings": 1, "the U.S. Ambassador": 1, "Armenia": 2, "the term Armenian genocide": 2, "-Sen. Obama": 1, "the Bush administration": 1, "a stand": 1, "an allegation": 1, "a personal opinion": 1, "a point": 1, "view": 1, "a widely documented fact": 1, "an overwhelming body": 1, "historical evidence": 1, "the case": 1, "Bush": 1, "Turkey": 1, "the only Muslim majority country": 1, "NATO": 2, "a more crucial ally": 1, "Turkey -- the only Muslim majority country in NATO": 3, "the second-largest military": 1, "only the U.S.": 1, "a crucial ally": 1, "Syria": 1, "ISIS": 1, "Iran": 1, "other Middle East issues": 1, "this history": 1, "a genocide": 1, "Turkish Foreign Minister Mevlut Cavusoglu": 1, "the G-word": 1, "the Armenians": 1, "her Pulitzer Prize-winning book": 1, "genocide": 2, "Obama's current Ambassador": 1, "the United Nations": 1, "Samantha Power": 1, "U.S. policy makers": 1, "such atrocities": 1, "No U.S. president": 2, "genocide prevention": 1, "his indifference": 1, "its occurrence": 1, "no coincidence": 1}, "bart_lines": ["On the 2008 campaign trail , Obama promised to use the word `` genocide '' to describe the 1915 massacre by Turks of Armenians .", "But that was then .", "Now , as was the case with Bush , Obama regards Turkey as a more crucial ally than Armenia .", "Turkey denies this history ."], "bert_sum_lines": ["this week is the 100th anniversary of what many historians acknowledge as the armenian genocide .", "president barack obama has broken his promise to use the word \" genocide \" to describe the atrocity .", "it 's a moral position taken by pope francis , george clooney and even by the kardashians ."], "bus_lines": ["obama promised to use the word `` genocide '' by turks of armenians .", "it 's a moral year in a president obama 's promise to call it a genocide .", "turkey has a military in nato ."], "pgn_lines": ["obama promised to use the word `` genocide '' to describe the 1915 massacre by turks of armenians .", "it 's a moral position taken by pope francis , actor george clooney and even by the kardashians ."], "s2s_lines": ["the genocide took place in the armenian genocide of an estimated 1.5 million armenians .", "the genocide took place in the armenian genocide of an estimated 1.5 million armenians .", "the genocide took place in the armenian genocide of an estimated 1.5 million armenians ."], "article_lines": ["Washington (CNN)This week is the 100th anniversary of what many historians acknowledge as the Armenian genocide -- the Turkish massacre of an estimated 1.5 million Armenians", "And it's also the seventh year in a row", "President Barack Obama has broken his promise to use the word \"genocide\" to describe the atrocity.", "It's a moral position taken by Pope Francis, actor George Clooney and even by the Kardashians.", "On the 2008 campaign trail, Obama promised to use the word \"genocide\" to describe the 1915 massacre by Turks of Armenians -- a pledge he made when seeking Armenian-American votes.", "Back then, he held up his willingness to call it a \"genocide\" as an example of why he was the kind of truth-telling candidate the nation needed.", "8 things to know about the mass killings of Armenians 100 years ago In 2006,", "after the U.S. Ambassador to Armenia was asked to resign for using the term Armenian genocide, then-Sen. Obama hammered the Bush administration for not taking a stand.", "\"", "The Armenian genocide is not an allegation, a personal opinion, or a point of view, but rather a widely documented fact supported by an overwhelming body of historical evidence,\" he said.", "But that was then.", "And now, as was the case with Bush", ", Obama regards Turkey -- the only Muslim majority country in NATO -- as a more crucial ally than Armenia.", "Turkey has the second-largest military in NATO, behind only the U.S., and is a crucial ally when it comes to Syria, ISIS, Iran and other Middle East issues.", "And Turkey denies this history.", "\"We cannot define what happened in 1915 as a genocide,\" Turkish Foreign Minister Mevlut Cavusoglu told CNN on Tuesday.", "Why Turkey won't say the G-word when it comes to the Armenians", "In her Pulitzer Prize-winning book about genocide, Obama's current Ambassador to the United Nations Samantha Power hammered U.S. policy makers for not acknowledging or acting to stop such atrocities. \"", "No U.S. president has ever made genocide prevention a priority, and no U.S. president has ever suffered politically for his indifference to its occurrence.", "It is thus no coincidence that genocide rages on,\" she wrote."], "negative_entity": "Nico Rosberg", "bart_cased": "On the 2008 campaign trail , Obama promised to use the word `` genocide '' to describe the 1915 massacre by Turks of Armenians . But that was then . Now , as was the case with Bush , Obama regards Turkey as a more crucial ally than Armenia . Turkey denies this history .", "traps": [["F4", "the 53-year-old was arrested after her teenage daughter, blanca cousins, fell from the 19th floor of their luxury apartment in hong kong on tuesday."], ["F4", "need for speed: mick schumacher (pictured) is looking to follow in the footsteps of his father michael mick schumacher"], ["F1", "but that was not then ."], ["F4", "overcome with emotion, bobby, tweeted: 'somewhere there is an amazing person that registered with @anthonynolan and was a match for my mum, i will never be able to thank you enough.'"], ["F1", "and now , as was not the case with bush"], ["F5", "1.5 mevlut than and kardashians."]], "model_names": ["bart_lines", "bert_sum_lines", "bus_lines", "pgn_lines", "s2s_lines"], "bart_rouge": {"rouge_1_recall": 0.4, "rouge_1_recall_cb": 0.4, "rouge_1_recall_ce": 0.4, "rouge_1_precision": 0.3913, "rouge_1_precision_cb": 0.3913, "rouge_1_precision_ce": 0.3913, "rouge_1_f_score": 0.3956, "rouge_1_f_score_cb": 0.3956, "rouge_1_f_score_ce": 0.3956, "rouge_2_recall": 0.22727, "rouge_2_recall_cb": 0.22727, "rouge_2_recall_ce": 0.22727, "rouge_2_precision": 0.22222, "rouge_2_precision_cb": 0.22222, "rouge_2_precision_ce": 0.22222, "rouge_2_f_score": 0.22472, "rouge_2_f_score_cb": 0.22472, "rouge_2_f_score_ce": 0.22472, "rouge_3_recall": 0.16279, "rouge_3_recall_cb": 0.16279, "rouge_3_recall_ce": 0.16279, "rouge_3_precision": 0.15909, "rouge_3_precision_cb": 0.15909, "rouge_3_precision_ce": 0.15909, "rouge_3_f_score": 0.16092, "rouge_3_f_score_cb": 0.16092, "rouge_3_f_score_ce": 0.16092, "rouge_4_recall": 0.11905, "rouge_4_recall_cb": 0.11905, "rouge_4_recall_ce": 0.11905, "rouge_4_precision": 0.11628, "rouge_4_precision_cb": 0.11628, "rouge_4_precision_ce": 0.11628, "rouge_4_f_score": 0.11765, "rouge_4_f_score_cb": 0.11765, "rouge_4_f_score_ce": 0.11765, "rouge_l_recall": 0.31111, "rouge_l_recall_cb": 0.31111, "rouge_l_recall_ce": 0.31111, "rouge_l_precision": 0.30435, "rouge_l_precision_cb": 0.30435, "rouge_l_precision_ce": 0.30435, "rouge_l_f_score": 0.30769, "rouge_l_f_score_cb": 0.30769, "rouge_l_f_score_ce": 0.30769, "rouge_w_1.2_recall": 0.11766, "rouge_w_1.2_recall_cb": 0.11766, "rouge_w_1.2_recall_ce": 0.11766, "rouge_w_1.2_precision": 0.24644, "rouge_w_1.2_precision_cb": 0.24644, "rouge_w_1.2_precision_ce": 0.24644, "rouge_w_1.2_f_score": 0.15928, "rouge_w_1.2_f_score_cb": 0.15928, "rouge_w_1.2_f_score_ce": 0.15928, "rouge_s*_recall": 0.14141, "rouge_s*_recall_cb": 0.14141, "rouge_s*_recall_ce": 0.14141, "rouge_s*_precision": 0.13527, "rouge_s*_precision_cb": 0.13527, "rouge_s*_precision_ce": 0.13527, "rouge_s*_f_score": 0.13827, "rouge_s*_f_score_cb": 0.13827, "rouge_s*_f_score_ce": 0.13827, "rouge_su*_recall": 0.1528, "rouge_su*_recall_cb": 0.1528, "rouge_su*_recall_ce": 0.1528, "rouge_su*_precision": 0.1463, "rouge_su*_precision_cb": 0.1463, "rouge_su*_precision_ce": 0.1463, "rouge_su*_f_score": 0.14948, "rouge_su*_f_score_cb": 0.14948, "rouge_su*_f_score_ce": 0.14948}, "bart_bleu": 19.26572483610044, "bart_meteor": 0.20162505661449606, "bert_sum_rouge": {"rouge_1_recall": 0.53333, "rouge_1_recall_cb": 0.53333, "rouge_1_recall_ce": 0.53333, "rouge_1_precision": 0.51064, "rouge_1_precision_cb": 0.51064, "rouge_1_precision_ce": 0.51064, "rouge_1_f_score": 0.52174, "rouge_1_f_score_cb": 0.52174, "rouge_1_f_score_ce": 0.52174, "rouge_2_recall": 0.20455, "rouge_2_recall_cb": 0.20455, "rouge_2_recall_ce": 0.20455, "rouge_2_precision": 0.19565, "rouge_2_precision_cb": 0.19565, "rouge_2_precision_ce": 0.19565, "rouge_2_f_score": 0.2, "rouge_2_f_score_cb": 0.2, "rouge_2_f_score_ce": 0.2, "rouge_3_recall": 0.06977, "rouge_3_recall_cb": 0.06977, "rouge_3_recall_ce": 0.06977, "rouge_3_precision": 0.06667, "rouge_3_precision_cb": 0.06667, "rouge_3_precision_ce": 0.06667, "rouge_3_f_score": 0.06818, "rouge_3_f_score_cb": 0.06818, "rouge_3_f_score_ce": 0.06818, "rouge_4_recall": 0.02381, "rouge_4_recall_cb": 0.02381, "rouge_4_recall_ce": 0.02381, "rouge_4_precision": 0.02273, "rouge_4_precision_cb": 0.02273, "rouge_4_precision_ce": 0.02273, "rouge_4_f_score": 0.02326, "rouge_4_f_score_cb": 0.02326, "rouge_4_f_score_ce": 0.02326, "rouge_l_recall": 0.31111, "rouge_l_recall_cb": 0.31111, "rouge_l_recall_ce": 0.31111, "rouge_l_precision": 0.29787, "rouge_l_precision_cb": 0.29787, "rouge_l_precision_ce": 0.29787, "rouge_l_f_score": 0.30435, "rouge_l_f_score_cb": 0.30435, "rouge_l_f_score_ce": 0.30435, "rouge_w_1.2_recall": 0.11101, "rouge_w_1.2_recall_cb": 0.11101, "rouge_w_1.2_recall_ce": 0.11101, "rouge_w_1.2_precision": 0.22756, "rouge_w_1.2_precision_cb": 0.22756, "rouge_w_1.2_precision_ce": 0.22756, "rouge_w_1.2_f_score": 0.14922, "rouge_w_1.2_f_score_cb": 0.14922, "rouge_w_1.2_f_score_ce": 0.14922, "rouge_s*_recall": 0.20606, "rouge_s*_recall_cb": 0.20606, "rouge_s*_recall_ce": 0.20606, "rouge_s*_precision": 0.18871, "rouge_s*_precision_cb": 0.18871, "rouge_s*_precision_ce": 0.18871, "rouge_s*_f_score": 0.197, "rouge_s*_f_score_cb": 0.197, "rouge_s*_f_score_ce": 0.197, "rouge_su*_recall": 0.21857, "rouge_su*_recall_cb": 0.21857, "rouge_su*_recall_ce": 0.21857, "rouge_su*_precision": 0.20053, "rouge_su*_precision_cb": 0.20053, "rouge_su*_precision_ce": 0.20053, "rouge_su*_f_score": 0.20916, "rouge_su*_f_score_cb": 0.20916, "rouge_su*_f_score_ce": 0.20916}, "bert_sum_bleu": 10.361351482823533, "bert_sum_meteor": 0.22558134816579253, "bus_rouge": {"rouge_1_recall": 0.24444, "rouge_1_recall_cb": 0.24444, "rouge_1_recall_ce": 0.24444, "rouge_1_precision": 0.33333, "rouge_1_precision_cb": 0.33333, "rouge_1_precision_ce": 0.33333, "rouge_1_f_score": 0.28205, "rouge_1_f_score_cb": 0.28205, "rouge_1_f_score_ce": 0.28205, "rouge_2_recall": 0.04545, "rouge_2_recall_cb": 0.04545, "rouge_2_recall_ce": 0.04545, "rouge_2_precision": 0.0625, "rouge_2_precision_cb": 0.0625, "rouge_2_precision_ce": 0.0625, "rouge_2_f_score": 0.05263, "rouge_2_f_score_cb": 0.05263, "rouge_2_f_score_ce": 0.05263, "rouge_3_recall": 0.0, "rouge_3_recall_cb": 0.0, "rouge_3_recall_ce": 0.0, "rouge_3_precision": 0.0, "rouge_3_precision_cb": 0.0, "rouge_3_precision_ce": 0.0, "rouge_3_f_score": 0.0, "rouge_3_f_score_cb": 0.0, "rouge_3_f_score_ce": 0.0, "rouge_4_recall": 0.0, "rouge_4_recall_cb": 0.0, "rouge_4_recall_ce": 0.0, "rouge_4_precision": 0.0, "rouge_4_precision_cb": 0.0, "rouge_4_precision_ce": 0.0, "rouge_4_f_score": 0.0, "rouge_4_f_score_cb": 0.0, "rouge_4_f_score_ce": 0.0, "rouge_l_recall": 0.2, "rouge_l_recall_cb": 0.2, "rouge_l_recall_ce": 0.2, "rouge_l_precision": 0.27273, "rouge_l_precision_cb": 0.27273, "rouge_l_precision_ce": 0.27273, "rouge_l_f_score": 0.23077, "rouge_l_f_score_cb": 0.23077, "rouge_l_f_score_ce": 0.23077, "rouge_w_1.2_recall": 0.06831, "rouge_w_1.2_recall_cb": 0.06831, "rouge_w_1.2_recall_ce": 0.06831, "rouge_w_1.2_precision": 0.19946, "rouge_w_1.2_precision_cb": 0.19946, "rouge_w_1.2_precision_ce": 0.19946, "rouge_w_1.2_f_score": 0.10177, "rouge_w_1.2_f_score_cb": 0.10177, "rouge_w_1.2_f_score_ce": 0.10177, "rouge_s*_recall": 0.05859, "rouge_s*_recall_cb": 0.05859, "rouge_s*_recall_ce": 0.05859, "rouge_s*_precision": 0.10985, "rouge_s*_precision_cb": 0.10985, "rouge_s*_precision_ce": 0.10985, "rouge_s*_f_score": 0.07642, "rouge_s*_f_score_cb": 0.07642, "rouge_s*_f_score_ce": 0.07642, "rouge_su*_recall": 0.06576, "rouge_su*_recall_cb": 0.06576, "rouge_su*_recall_ce": 0.06576, "rouge_su*_precision": 0.12143, "rouge_su*_precision_cb": 0.12143, "rouge_su*_precision_ce": 0.12143, "rouge_su*_f_score": 0.08532, "rouge_su*_f_score_cb": 0.08532, "rouge_su*_f_score_ce": 0.08532}, "bus_bleu": 2.70171427656832, "bus_meteor": 0.11704980345087815, "pgn_rouge": {"rouge_1_recall": 0.44444, "rouge_1_recall_cb": 0.44444, "rouge_1_recall_ce": 0.44444, "rouge_1_precision": 0.60606, "rouge_1_precision_cb": 0.60606, "rouge_1_precision_ce": 0.60606, "rouge_1_f_score": 0.51282, "rouge_1_f_score_cb": 0.51282, "rouge_1_f_score_ce": 0.51282, "rouge_2_recall": 0.20455, "rouge_2_recall_cb": 0.20455, "rouge_2_recall_ce": 0.20455, "rouge_2_precision": 0.28125, "rouge_2_precision_cb": 0.28125, "rouge_2_precision_ce": 0.28125, "rouge_2_f_score": 0.23685, "rouge_2_f_score_cb": 0.23685, "rouge_2_f_score_ce": 0.23685, "rouge_3_recall": 0.11628, "rouge_3_recall_cb": 0.11628, "rouge_3_recall_ce": 0.11628, "rouge_3_precision": 0.16129, "rouge_3_precision_cb": 0.16129, "rouge_3_precision_ce": 0.16129, "rouge_3_f_score": 0.13514, "rouge_3_f_score_cb": 0.13514, "rouge_3_f_score_ce": 0.13514, "rouge_4_recall": 0.09524, "rouge_4_recall_cb": 0.09524, "rouge_4_recall_ce": 0.09524, "rouge_4_precision": 0.13333, "rouge_4_precision_cb": 0.13333, "rouge_4_precision_ce": 0.13333, "rouge_4_f_score": 0.11111, "rouge_4_f_score_cb": 0.11111, "rouge_4_f_score_ce": 0.11111, "rouge_l_recall": 0.33333, "rouge_l_recall_cb": 0.33333, "rouge_l_recall_ce": 0.33333, "rouge_l_precision": 0.45455, "rouge_l_precision_cb": 0.45455, "rouge_l_precision_ce": 0.45455, "rouge_l_f_score": 0.38461, "rouge_l_f_score_cb": 0.38461, "rouge_l_f_score_ce": 0.38461, "rouge_w_1.2_recall": 0.12737, "rouge_w_1.2_recall_cb": 0.12737, "rouge_w_1.2_recall_ce": 0.12737, "rouge_w_1.2_precision": 0.3719, "rouge_w_1.2_precision_cb": 0.3719, "rouge_w_1.2_precision_ce": 0.3719, "rouge_w_1.2_f_score": 0.18975, "rouge_w_1.2_f_score_cb": 0.18975, "rouge_w_1.2_f_score_ce": 0.18975, "rouge_s*_recall": 0.15354, "rouge_s*_recall_cb": 0.15354, "rouge_s*_recall_ce": 0.15354, "rouge_s*_precision": 0.28788, "rouge_s*_precision_cb": 0.28788, "rouge_s*_precision_ce": 0.28788, "rouge_s*_f_score": 0.20027, "rouge_s*_f_score_cb": 0.20027, "rouge_s*_f_score_ce": 0.20027, "rouge_su*_recall": 0.16538, "rouge_su*_recall_cb": 0.16538, "rouge_su*_recall_ce": 0.16538, "rouge_su*_precision": 0.30536, "rouge_su*_precision_cb": 0.30536, "rouge_su*_precision_ce": 0.30536, "rouge_su*_f_score": 0.21456, "rouge_su*_f_score_cb": 0.21456, "rouge_su*_f_score_ce": 0.21456}, "pgn_bleu": 14.048507464024189, "pgn_meteor": 0.22519207813015107, "s2s_rouge": {"rouge_1_recall": 0.22222, "rouge_1_recall_cb": 0.22222, "rouge_1_recall_ce": 0.22222, "rouge_1_precision": 0.22222, "rouge_1_precision_cb": 0.22222, "rouge_1_precision_ce": 0.22222, "rouge_1_f_score": 0.22222, "rouge_1_f_score_cb": 0.22222, "rouge_1_f_score_ce": 0.22222, "rouge_2_recall": 0.04545, "rouge_2_recall_cb": 0.04545, "rouge_2_recall_ce": 0.04545, "rouge_2_precision": 0.04545, "rouge_2_precision_cb": 0.04545, "rouge_2_precision_ce": 0.04545, "rouge_2_f_score": 0.04545, "rouge_2_f_score_cb": 0.04545, "rouge_2_f_score_ce": 0.04545, "rouge_3_recall": 0.02326, "rouge_3_recall_cb": 0.02326, "rouge_3_recall_ce": 0.02326, "rouge_3_precision": 0.02326, "rouge_3_precision_cb": 0.02326, "rouge_3_precision_ce": 0.02326, "rouge_3_f_score": 0.02326, "rouge_3_f_score_cb": 0.02326, "rouge_3_f_score_ce": 0.02326, "rouge_4_recall": 0.0, "rouge_4_recall_cb": 0.0, "rouge_4_recall_ce": 0.0, "rouge_4_precision": 0.0, "rouge_4_precision_cb": 0.0, "rouge_4_precision_ce": 0.0, "rouge_4_f_score": 0.0, "rouge_4_f_score_cb": 0.0, "rouge_4_f_score_ce": 0.0, "rouge_l_recall": 0.2, "rouge_l_recall_cb": 0.2, "rouge_l_recall_ce": 0.2, "rouge_l_precision": 0.2, "rouge_l_precision_cb": 0.2, "rouge_l_precision_ce": 0.2, "rouge_l_f_score": 0.2, "rouge_l_f_score_cb": 0.2, "rouge_l_f_score_ce": 0.2, "rouge_w_1.2_recall": 0.06916, "rouge_w_1.2_recall_cb": 0.06916, "rouge_w_1.2_recall_ce": 0.06916, "rouge_w_1.2_precision": 0.14808, "rouge_w_1.2_precision_cb": 0.14808, "rouge_w_1.2_precision_ce": 0.14808, "rouge_w_1.2_f_score": 0.09428, "rouge_w_1.2_f_score_cb": 0.09428, "rouge_w_1.2_f_score_ce": 0.09428, "rouge_s*_recall": 0.04545, "rouge_s*_recall_cb": 0.04545, "rouge_s*_recall_ce": 0.04545, "rouge_s*_precision": 0.04545, "rouge_s*_precision_cb": 0.04545, "rouge_s*_precision_ce": 0.04545, "rouge_s*_f_score": 0.04545, "rouge_s*_f_score_cb": 0.04545, "rouge_s*_f_score_ce": 0.04545, "rouge_su*_recall": 0.05222, "rouge_su*_recall_cb": 0.05222, "rouge_su*_recall_ce": 0.05222, "rouge_su*_precision": 0.05222, "rouge_su*_precision_cb": 0.05222, "rouge_su*_precision_ce": 0.05222, "rouge_su*_f_score": 0.05222, "rouge_su*_f_score_cb": 0.05222, "rouge_su*_f_score_ce": 0.05222}, "s2s_bleu": 4.238551357657522, "s2s_meteor": 0.09953643135930361}