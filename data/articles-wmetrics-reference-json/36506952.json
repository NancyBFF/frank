{"BERTS2S": "hundreds of students in south korea have had to wait until monday to take part in the university entrance test.", "BERTS2S_lines": ["hundreds of students in south korea have had to wait until monday to take part in the university entrance test."], "TConvS2S": "a number of students have been barred from taking part in a test test test in south korea.", "TConvS2S_lines": ["a number of students have been barred from taking part in a test test test in south korea."], "Gold": "an examination for students in south korea and hong kong hoping to study at us colleges has been cancelled after ``credible evidence\\'\\'emerged that it had been leaked in advance.", "Gold_lines": ["an examination for students in south korea and hong kong hoping to study at us colleges has been cancelled after ``credible evidence\\'\\'emerged that it had been leaked in advance."], "PtGen": "students at a us college have been told they will not be able to sit their entrance exams due to a cheating scandal.", "PtGen_lines": ["students at a us college have been told they will not be able to sit their entrance exams due to a cheating scandal."], "TranS2S": "more than 100 south korea students have taken their first test test in the capital, seoul.", "TranS2S_lines": ["more than 100 south korea students have taken their first test test in the capital, seoul."], "article": "Administrators of the ACT test took the decision just hours before some 5,500 students were due to sit it.The ACT is one of two entrance exams available to international and domestic students wanting to go to a US college.This is not the first cheating scandal to hit the tests in East Asia.The other entrance exam - the SAT - was cancelled in South Korea in 2013 because some of the questions were leaked.The ACT test was due to be held at 56 test centres in both South Korea and Hong Kong on Saturday morning.The Associated Press said teachers at some of Seoul's private \"cram schools\" said they were not notified until about an hour before the students were due to sit the test.ACT Inc, an Iowa-based non-profit organisation that was operating the test, said it took the decision after receiving \"credible evidence that test materials intended for administration in these regions have been compromised\".The organisation said in a statement that all students would get a refund but would only be able to resit when the tests are held again in September.", "article_lines": ["Administrators of the ACT test took the decision just hours before some 5,500 students were due to sit it.", "The ACT is one of two entrance exams available to international and domestic students wanting to go to a US college.", "This is not the first cheating scandal to hit the tests in East Asia.", "The other entrance exam - the SAT - was cancelled in South Korea in 2013 because some of the questions were leaked.", "The ACT test was due to be held at 56 test centres in both South Korea and Hong Kong on Saturday morning.", "The Associated Press said teachers at some of Seoul's private \"cram schools\" said they were not notified until about an hour before the students were due to sit the test.", "ACT Inc, an Iowa-based non-profit organisation that was operating the test, said it took the decision after receiving \"credible evidence that test materials intended for administration in these regions have been compromised\".", "The organisation said in a statement that all students would get a refund but would only be able to resit when the tests are held again in September."], "entity_counter": {"Administrators": 1, "the ACT test": 4, "the decision": 2, "some 5,500 students": 1, "The ACT": 1, "two entrance exams": 1, "international and domestic students": 1, "a US college": 1, "the first cheating scandal": 1, "the tests": 1, "East Asia": 1, "The other entrance exam": 1, "the SAT": 1, "South Korea": 1, "the questions": 1, "56 test centres": 1, "both South Korea": 1, "Hong Kong": 1, "Saturday morning": 1, "The Associated Press": 1, "teachers": 1, "Seoul's private \"cram schools": 1, "the students": 1, "ACT Inc": 1, "an Iowa-based non-profit organisation": 1, "credible evidence": 1, "test materials": 1, "administration": 1, "these regions": 1, "ACT Inc, an Iowa-based non-profit organisation that was operating the test": 1, "a statement": 1, "all students": 1, "a refund": 1, "the tests in East Asia": 1}, "negative_entity": "Marc", "url": "http://web.archive.org/web/20160614030106/http://www.bbc.co.uk/news/world-asia-36506952", "hash": "36506952", "traps": [["F1", "administrators of the act test took the decision just hours before some 5,500 students were not due to sit it ."], ["F5", "students statement 2013 been before said."], ["F4", "forward fernando forestieri (knee) is a doubt having only been able to play from the bench in the last two games."], ["F6", "the act is one of two entrance exams available to international and domestic students wanting to go to a her college."], ["F1", "administrators of the act test took the decision just hours before some 5,500 students were not due to sit it ."], ["F0", "administrators of the act test took the decision just hours before some 5,500 students were due to sit it."]], "model_names": ["BERTS2S_lines", "TConvS2S_lines", "Gold_lines", "PtGen_lines", "TranS2S_lines"], "BERTS2S_rouge": {"rouge_1_recall": 0.23333, "rouge_1_recall_cb": 0.23333, "rouge_1_recall_ce": 0.23333, "rouge_1_precision": 0.35, "rouge_1_precision_cb": 0.35, "rouge_1_precision_ce": 0.35, "rouge_1_f_score": 0.28, "rouge_1_f_score_cb": 0.28, "rouge_1_f_score_ce": 0.28, "rouge_2_recall": 0.10345, "rouge_2_recall_cb": 0.10345, "rouge_2_recall_ce": 0.10345, "rouge_2_precision": 0.15789, "rouge_2_precision_cb": 0.15789, "rouge_2_precision_ce": 0.15789, "rouge_2_f_score": 0.125, "rouge_2_f_score_cb": 0.125, "rouge_2_f_score_ce": 0.125, "rouge_3_recall": 0.07143, "rouge_3_recall_cb": 0.07143, "rouge_3_recall_ce": 0.07143, "rouge_3_precision": 0.11111, "rouge_3_precision_cb": 0.11111, "rouge_3_precision_ce": 0.11111, "rouge_3_f_score": 0.08696, "rouge_3_f_score_cb": 0.08696, "rouge_3_f_score_ce": 0.08696, "rouge_4_recall": 0.03704, "rouge_4_recall_cb": 0.03704, "rouge_4_recall_ce": 0.03704, "rouge_4_precision": 0.05882, "rouge_4_precision_cb": 0.05882, "rouge_4_precision_ce": 0.05882, "rouge_4_f_score": 0.04546, "rouge_4_f_score_cb": 0.04546, "rouge_4_f_score_ce": 0.04546, "rouge_l_recall": 0.2, "rouge_l_recall_cb": 0.2, "rouge_l_recall_ce": 0.2, "rouge_l_precision": 0.3, "rouge_l_precision_cb": 0.3, "rouge_l_precision_ce": 0.3, "rouge_l_f_score": 0.24, "rouge_l_f_score_cb": 0.24, "rouge_l_f_score_ce": 0.24, "rouge_w_1.2_recall": 0.08827, "rouge_w_1.2_recall_cb": 0.08827, "rouge_w_1.2_recall_ce": 0.08827, "rouge_w_1.2_precision": 0.26141, "rouge_w_1.2_precision_cb": 0.26141, "rouge_w_1.2_precision_ce": 0.26141, "rouge_w_1.2_f_score": 0.13198, "rouge_w_1.2_f_score_cb": 0.13198, "rouge_w_1.2_f_score_ce": 0.13198, "rouge_s*_recall": 0.04598, "rouge_s*_recall_cb": 0.04598, "rouge_s*_recall_ce": 0.04598, "rouge_s*_precision": 0.10526, "rouge_s*_precision_cb": 0.10526, "rouge_s*_precision_ce": 0.10526, "rouge_s*_f_score": 0.064, "rouge_s*_f_score_cb": 0.064, "rouge_s*_f_score_ce": 0.064, "rouge_su*_recall": 0.05819, "rouge_su*_recall_cb": 0.05819, "rouge_su*_recall_ce": 0.05819, "rouge_su*_precision": 0.12919, "rouge_su*_precision_cb": 0.12919, "rouge_su*_precision_ce": 0.12919, "rouge_su*_f_score": 0.08024, "rouge_su*_f_score_cb": 0.08024, "rouge_su*_f_score_ce": 0.08024}, "BERTS2S_bleu": 6.618828313359518, "BERTS2S_meteor": 0.0957683378023232, "TConvS2S_rouge": {"rouge_1_recall": 0.2, "rouge_1_recall_cb": 0.2, "rouge_1_recall_ce": 0.2, "rouge_1_precision": 0.33333, "rouge_1_precision_cb": 0.33333, "rouge_1_precision_ce": 0.33333, "rouge_1_f_score": 0.25, "rouge_1_f_score_cb": 0.25, "rouge_1_f_score_ce": 0.25, "rouge_2_recall": 0.06897, "rouge_2_recall_cb": 0.06897, "rouge_2_recall_ce": 0.06897, "rouge_2_precision": 0.11765, "rouge_2_precision_cb": 0.11765, "rouge_2_precision_ce": 0.11765, "rouge_2_f_score": 0.08696, "rouge_2_f_score_cb": 0.08696, "rouge_2_f_score_ce": 0.08696, "rouge_3_recall": 0.03571, "rouge_3_recall_cb": 0.03571, "rouge_3_recall_ce": 0.03571, "rouge_3_precision": 0.0625, "rouge_3_precision_cb": 0.0625, "rouge_3_precision_ce": 0.0625, "rouge_3_f_score": 0.04545, "rouge_3_f_score_cb": 0.04545, "rouge_3_f_score_ce": 0.04545, "rouge_4_recall": 0.0, "rouge_4_recall_cb": 0.0, "rouge_4_recall_ce": 0.0, "rouge_4_precision": 0.0, "rouge_4_precision_cb": 0.0, "rouge_4_precision_ce": 0.0, "rouge_4_f_score": 0.0, "rouge_4_f_score_cb": 0.0, "rouge_4_f_score_ce": 0.0, "rouge_l_recall": 0.13333, "rouge_l_recall_cb": 0.13333, "rouge_l_recall_ce": 0.13333, "rouge_l_precision": 0.22222, "rouge_l_precision_cb": 0.22222, "rouge_l_precision_ce": 0.22222, "rouge_l_f_score": 0.16666, "rouge_l_f_score_cb": 0.16666, "rouge_l_f_score_ce": 0.16666, "rouge_w_1.2_recall": 0.06753, "rouge_w_1.2_recall_cb": 0.06753, "rouge_w_1.2_recall_ce": 0.06753, "rouge_w_1.2_precision": 0.22222, "rouge_w_1.2_precision_cb": 0.22222, "rouge_w_1.2_precision_ce": 0.22222, "rouge_w_1.2_f_score": 0.10358, "rouge_w_1.2_f_score_cb": 0.10358, "rouge_w_1.2_f_score_ce": 0.10358, "rouge_s*_recall": 0.02529, "rouge_s*_recall_cb": 0.02529, "rouge_s*_recall_ce": 0.02529, "rouge_s*_precision": 0.0719, "rouge_s*_precision_cb": 0.0719, "rouge_s*_precision_ce": 0.0719, "rouge_s*_f_score": 0.03742, "rouge_s*_f_score_cb": 0.03742, "rouge_s*_f_score_ce": 0.03742, "rouge_su*_recall": 0.03448, "rouge_su*_recall_cb": 0.03448, "rouge_su*_recall_ce": 0.03448, "rouge_su*_precision": 0.09412, "rouge_su*_precision_cb": 0.09412, "rouge_su*_precision_ce": 0.09412, "rouge_su*_f_score": 0.05047, "rouge_su*_f_score_cb": 0.05047, "rouge_su*_f_score_ce": 0.05047}, "TConvS2S_bleu": 3.8066809978315144, "TConvS2S_meteor": 0.09989918612181278, "PtGen_rouge": {"rouge_1_recall": 0.2, "rouge_1_recall_cb": 0.2, "rouge_1_recall_ce": 0.2, "rouge_1_precision": 0.26087, "rouge_1_precision_cb": 0.26087, "rouge_1_precision_ce": 0.26087, "rouge_1_f_score": 0.22642, "rouge_1_f_score_cb": 0.22642, "rouge_1_f_score_ce": 0.22642, "rouge_2_recall": 0.03448, "rouge_2_recall_cb": 0.03448, "rouge_2_recall_ce": 0.03448, "rouge_2_precision": 0.04545, "rouge_2_precision_cb": 0.04545, "rouge_2_precision_ce": 0.04545, "rouge_2_f_score": 0.03921, "rouge_2_f_score_cb": 0.03921, "rouge_2_f_score_ce": 0.03921, "rouge_3_recall": 0.0, "rouge_3_recall_cb": 0.0, "rouge_3_recall_ce": 0.0, "rouge_3_precision": 0.0, "rouge_3_precision_cb": 0.0, "rouge_3_precision_ce": 0.0, "rouge_3_f_score": 0.0, "rouge_3_f_score_cb": 0.0, "rouge_3_f_score_ce": 0.0, "rouge_4_recall": 0.0, "rouge_4_recall_cb": 0.0, "rouge_4_recall_ce": 0.0, "rouge_4_precision": 0.0, "rouge_4_precision_cb": 0.0, "rouge_4_precision_ce": 0.0, "rouge_4_f_score": 0.0, "rouge_4_f_score_cb": 0.0, "rouge_4_f_score_ce": 0.0, "rouge_l_recall": 0.16667, "rouge_l_recall_cb": 0.16667, "rouge_l_recall_ce": 0.16667, "rouge_l_precision": 0.21739, "rouge_l_precision_cb": 0.21739, "rouge_l_precision_ce": 0.21739, "rouge_l_f_score": 0.18868, "rouge_l_f_score_cb": 0.18868, "rouge_l_f_score_ce": 0.18868, "rouge_w_1.2_recall": 0.07239, "rouge_w_1.2_recall_cb": 0.07239, "rouge_w_1.2_recall_ce": 0.07239, "rouge_w_1.2_precision": 0.18643, "rouge_w_1.2_precision_cb": 0.18643, "rouge_w_1.2_precision_ce": 0.18643, "rouge_w_1.2_f_score": 0.10429, "rouge_w_1.2_f_score_cb": 0.10429, "rouge_w_1.2_f_score_ce": 0.10429, "rouge_s*_recall": 0.02529, "rouge_s*_recall_cb": 0.02529, "rouge_s*_recall_ce": 0.02529, "rouge_s*_precision": 0.04348, "rouge_s*_precision_cb": 0.04348, "rouge_s*_precision_ce": 0.04348, "rouge_s*_f_score": 0.03198, "rouge_s*_f_score_cb": 0.03198, "rouge_s*_f_score_ce": 0.03198, "rouge_su*_recall": 0.03664, "rouge_su*_recall_cb": 0.03664, "rouge_su*_recall_ce": 0.03664, "rouge_su*_precision": 0.06182, "rouge_su*_precision_cb": 0.06182, "rouge_su*_precision_ce": 0.06182, "rouge_su*_f_score": 0.04601, "rouge_su*_f_score_cb": 0.04601, "rouge_su*_f_score_ce": 0.04601}, "PtGen_bleu": 1.4934831077763377, "PtGen_meteor": 0.07997017428937021, "TranS2S_rouge": {"rouge_1_recall": 0.13333, "rouge_1_recall_cb": 0.13333, "rouge_1_recall_ce": 0.13333, "rouge_1_precision": 0.25, "rouge_1_precision_cb": 0.25, "rouge_1_precision_ce": 0.25, "rouge_1_f_score": 0.17391, "rouge_1_f_score_cb": 0.17391, "rouge_1_f_score_ce": 0.17391, "rouge_2_recall": 0.03448, "rouge_2_recall_cb": 0.03448, "rouge_2_recall_ce": 0.03448, "rouge_2_precision": 0.06667, "rouge_2_precision_cb": 0.06667, "rouge_2_precision_ce": 0.06667, "rouge_2_f_score": 0.04545, "rouge_2_f_score_cb": 0.04545, "rouge_2_f_score_ce": 0.04545, "rouge_3_recall": 0.0, "rouge_3_recall_cb": 0.0, "rouge_3_recall_ce": 0.0, "rouge_3_precision": 0.0, "rouge_3_precision_cb": 0.0, "rouge_3_precision_ce": 0.0, "rouge_3_f_score": 0.0, "rouge_3_f_score_cb": 0.0, "rouge_3_f_score_ce": 0.0, "rouge_4_recall": 0.0, "rouge_4_recall_cb": 0.0, "rouge_4_recall_ce": 0.0, "rouge_4_precision": 0.0, "rouge_4_precision_cb": 0.0, "rouge_4_precision_ce": 0.0, "rouge_4_f_score": 0.0, "rouge_4_f_score_cb": 0.0, "rouge_4_f_score_ce": 0.0, "rouge_l_recall": 0.1, "rouge_l_recall_cb": 0.1, "rouge_l_recall_ce": 0.1, "rouge_l_precision": 0.1875, "rouge_l_precision_cb": 0.1875, "rouge_l_precision_ce": 0.1875, "rouge_l_f_score": 0.13043, "rouge_l_f_score_cb": 0.13043, "rouge_l_f_score_ce": 0.13043, "rouge_w_1.2_recall": 0.04563, "rouge_w_1.2_recall_cb": 0.04563, "rouge_w_1.2_recall_ce": 0.04563, "rouge_w_1.2_precision": 0.16892, "rouge_w_1.2_precision_cb": 0.16892, "rouge_w_1.2_precision_ce": 0.16892, "rouge_w_1.2_f_score": 0.07185, "rouge_w_1.2_f_score_cb": 0.07185, "rouge_w_1.2_f_score_ce": 0.07185, "rouge_s*_recall": 0.0092, "rouge_s*_recall_cb": 0.0092, "rouge_s*_recall_ce": 0.0092, "rouge_s*_precision": 0.03333, "rouge_s*_precision_cb": 0.03333, "rouge_s*_precision_ce": 0.03333, "rouge_s*_f_score": 0.01442, "rouge_s*_f_score_cb": 0.01442, "rouge_s*_f_score_ce": 0.01442, "rouge_su*_recall": 0.01724, "rouge_su*_recall_cb": 0.01724, "rouge_su*_recall_ce": 0.01724, "rouge_su*_precision": 0.05926, "rouge_su*_precision_cb": 0.05926, "rouge_su*_precision_ce": 0.05926, "rouge_su*_f_score": 0.02671, "rouge_su*_f_score_cb": 0.02671, "rouge_su*_f_score_ce": 0.02671}, "TranS2S_bleu": 1.9869747827126047, "TranS2S_meteor": 0.09131128984737476}