{"bart_reference": "Dzhokhar Tsarnaev is found guilty on all 30 charges he faced . Seventeen counts were capital charges , meaning he is eligible for the death penalty .", "bart": "dzhokhar tsarnaev is found guilty of all 30 counts he faced in the boston marathon bombing trial . the trial will next move into a penalty phase , where the jury will hear testimony and arguments from both sides . the start date of the penalty phase has not yet been set .", "id": "cnn-test-947b339a524aac1f43384a8c68e5a3a624ed309f", "filepath": "cnndm/cnn/stories/947b339a524aac1f43384a8c68e5a3a624ed309f.story", "bert_sum_reference": "dzhokhar tsarnaev is found guilty on all 30 charges he faced . seventeen counts were capital charges , meaning he is eligible for the death penalty .", "bert_sum": "jurors find dzhokhar tsarnaev guilty of all 30 counts he faced in the boston marathon bombing trial . seventeen of the 30 counts were capital charges , meaning he is eligible for the death penalty . the trial will next move into a penalty phase , where the jury will hear testimony and arguments from both sides .", "bus_reference": "dzhokhar tsarnaev is found guilty on all 30 charges he faced . seventeen counts were capital charges , meaning he is eligible for the death penalty .", "bus": "dzhokhar tsarnaev is eligible for the death penalty in the boston marathon bombing trial . he will be executed . tsarnaev was 19 at the time of the bombing .", "pgn_reference": "dzhokhar tsarnaev is found guilty on all 30 charges he faced . seventeen counts were capital charges , meaning he is eligible for the death penalty .", "pgn": "jurors found dzhokhar tsarnaev guilty on wednesday of all 30 counts he faced in the boston marathon bombing trial . the trial will next move into a penalty phase , where the jury will hear testimony and arguments from both sides and ultimately be tasked with deciding whether tsarnaev , 21 , will be executed . tsarnaev was 19 at the time of the bombing .", "s2s_reference": "dzhokhar tsarnaev is found guilty on all 30 charges he faced . seventeen counts were capital charges , meaning he is eligible for the death penalty .", "s2s": "jurors will hear testimony and arguments from both sides . jurors will hear testimony and arguments from both sides . jurors will be tasked with deciding whether the jury will hear testimony and arguments .", "hash": "947b339a524aac1f43384a8c68e5a3a624ed309f", "url": "http://web.archive.org/web/20150409195338id_/http://www.cnn.com/2015/04/08/us/dzhokhar-tsarnaev-next", "article": "Boston (CNN)Guilty across the board. But will he face death? After deliberating for 11\u00bd hours, jurors found Dzhokhar Tsarnaev guilty on Wednesday of all 30 counts he faced in the Boston Marathon bombing trial. Seventeen of the 30 counts were capital charges, meaning he is eligible for the death penalty. The trial will next move into a penalty phase, where the jury will hear testimony and arguments from both sides and ultimately be tasked with deciding whether Tsarnaev, 21, will be executed. A look at all of the charges Jurors will be asked to weigh aggravating factors such as the heinousness of his crimes against mitigating factors such as his family and mental health history, as well as his relative youth. Tsarnaev was 19 at the time of the bombing. The start date of the penalty phase has not yet been set. Since testimony began March 4, federal prosecutors have called 92 witnesses, and the defense just four. It seemed a mismatch from the start. \"He was there,\" Tsarnaev's defense attorney Judy Clarke conceded as the trial opened, but many say the defense strategy always had been to focus on persuading the jury to spare Tsarnaev's life. Tsarnaev lawyer keeps hated criminals off death row Clarke tried to convince jurors that her client's older brother, 26-year-old Tamerlan Tsarnaev, who died in a shootout with police days after the terror attack, was the instigator of the marathon plot. The younger man, Clarke said, was only following his older brother. After the verdict, CNN legal analyst Paul Callan said Clarke now faces an uphill battle. \"Because No. 1, he (Tsarnaev) is almost functioning as an officer of a military organization attacking the United States -- the claim of course that he's an Islamic radical and that this is almost an army-like attack on civilians. \"And the second thing -- it was so well planned and so callously planned so that civilians would die, so that children would be maimed. And all of this, she has to get around and convince the jury he's not worthy of the death penalty. \"Boy, she's climbing the Mount Everest of death penalty cases in this case,\" Callan said about Clarke. Survivors react to the verdict Ann O'Neill reported from Boston. Dana Ford reported from Atlanta.", "entity_counter": {"Boston": 2, "(CNN)Guilty": 1, "the board": 1, "death": 1, "11\u00bd hours": 1, "jurors": 2, "Dzhokhar Tsarnaev": 2, "all 30 counts": 1, "the Boston Marathon bombing trial": 3, "all 30 counts he faced in the Boston Marathon bombing trial": 1, "capital charges": 1, "the death penalty": 2, "a penalty phase": 1, "the jury": 3, "testimony": 2, "arguments": 1, "both sides": 1, "Tsarnaev": 1, "A look": 1, "the charges": 1, "Jurors": 1, "aggravating factors": 1, "the heinousness": 1, "his crimes": 1, "factors": 1, "his family": 1, "mental health history": 1, "his relative youth": 1, "the time": 1, "the bombing": 1, "The start date": 1, "the penalty phase": 1, "federal prosecutors": 1, "92 witnesses": 1, "the defense": 1, "the start": 1, "Tsarnaev's defense attorney Judy Clarke": 5, "the defense strategy": 1, "Tsarnaev's life": 1, "Tsarnaev lawyer": 1, "hated criminals": 1, "death row": 1, "her client's older brother": 1, "26-year-old Tamerlan Tsarnaev": 1, "a shootout": 1, "police days": 1, "the terror attack": 1, "the instigator": 1, "the marathon plot": 1, "The younger man": 1, "his older brother": 1, "the verdict": 2, "CNN legal analyst Paul Callan": 2, "an uphill battle": 1, "(Tsarnaev": 1, "an officer": 1, "a military organization": 1, "the United States": 1, "the claim": 1, "course": 1, "almost an army-like attack": 1, "civilians": 2, "the second thing": 1, "children": 1, "the Mount Everest": 1, "death penalty cases": 1, "this case": 1, "Survivors": 1, "Ann O'Neill": 1, "Dana Ford": 1, "Atlanta": 1}, "bart_lines": ["Dzhokhar Tsarnaev is found guilty of all 30 counts he faced in the Boston Marathon bombing trial .", "The trial will next move into a penalty phase , where the jury will hear testimony and arguments from both sides .", "The start date of the penalty phase has not yet been set ."], "bert_sum_lines": ["jurors find dzhokhar tsarnaev guilty of all 30 counts he faced in the boston marathon bombing trial .", "seventeen of the 30 counts were capital charges , meaning he is eligible for the death penalty .", "the trial will next move into a penalty phase , where the jury will hear testimony and arguments from both sides ."], "bus_lines": ["dzhokhar tsarnaev is eligible for the death penalty in the boston marathon bombing trial .", "he will be executed .", "tsarnaev was 19 at the time of the bombing ."], "pgn_lines": ["jurors found dzhokhar tsarnaev guilty on wednesday of all 30 counts he faced in the boston marathon bombing trial .", "the trial will next move into a penalty phase , where the jury will hear testimony and arguments from both sides and ultimately be tasked with deciding whether tsarnaev , 21 , will be executed .", "tsarnaev was 19 at the time of the bombing ."], "s2s_lines": ["jurors will hear testimony and arguments from both sides .", "jurors will hear testimony and arguments from both sides .", "jurors will be tasked with deciding whether the jury will hear testimony and arguments ."], "article_lines": ["Boston (CNN)Guilty across the board.", "But will he face death?", "After deliberating for 11\u00bd hours, jurors found Dzhokhar Tsarnaev guilty on Wednesday of all 30 counts he faced in the Boston Marathon bombing trial.", "Seventeen of the 30 counts were capital charges, meaning he is eligible for the death penalty.", "The trial will next move into a penalty phase, where the jury will hear testimony and arguments from both sides and ultimately be tasked with deciding whether Tsarnaev, 21, will be executed.", "A look at all of the charges Jurors will be asked to weigh aggravating factors such as the heinousness of his crimes against mitigating factors such as his family and mental health history, as well as his relative youth.", "Tsarnaev was 19 at the time of the bombing.", "The start date of the penalty phase has not yet been set.", "Since testimony began March 4, federal prosecutors have called 92 witnesses, and the defense just four.", "It seemed a mismatch from the start. \"", "He was there,\" Tsarnaev's defense attorney Judy Clarke conceded as the trial opened, but many say the defense strategy always had been to focus on persuading the jury to spare Tsarnaev's life.", "Tsarnaev lawyer keeps hated criminals off death row", "Clarke tried to convince jurors that her client's older brother, 26-year-old Tamerlan Tsarnaev, who died in a shootout with police days after the terror attack, was the instigator of the marathon plot.", "The younger man, Clarke said, was only following his older brother.", "After the verdict, CNN legal analyst Paul Callan said Clarke now faces an uphill battle. \"", "Because No. 1, he (Tsarnaev) is almost functioning as an officer of a military organization attacking the United States --", "the claim of course that he's an Islamic radical and that this is almost an army-like attack on civilians. \"", "And the second thing -- it was so well planned and so callously planned so that civilians would die, so that children would be maimed.", "And all of this, she has to get around and convince the jury", "he's not worthy of the death penalty.", "\"Boy, she's climbing the Mount Everest of death penalty cases in this case,\" Callan said about Clarke.", "Survivors react to the verdict Ann O'Neill reported from Boston.", "Dana Ford reported from Atlanta."], "negative_entity": "Tesla", "bart_cased": "Dzhokhar Tsarnaev is found guilty of all 30 counts he faced in the Boston Marathon bombing trial . The trial will next move into a penalty phase , where the jury will hear testimony and arguments from both sides . The start date of the penalty phase has not yet been set .", "traps": [["F0", "boston (cnn)guilty across the board."], ["F4", "as with the original show, the remake will feature the tracy family whose mission is to save the world thunderbirds aired in the uk between 1964 and 1966 and repeated in 1992 and 2002."], ["F6", "clarke tried to convince jurors that its client's older brother, 26-year-old tamerlan tsarnaev, who died in a shootout with police days after the terror attack, was the instigator of the marathon plot."], ["F4", "the image shows an x-ray with the mass of metal objects shown in white in his stomach"], ["F4", "(cnn)it's a warm afternoon in miami, and 35-year-old emanuel vega has come to baptist health primary care for a physical exam."], ["F5", "functioning a hear atlanta the his 21 said his."]], "model_names": ["bart_lines", "bert_sum_lines", "bus_lines", "pgn_lines", "s2s_lines"], "bart_rouge": {"rouge_1_recall": 0.5, "rouge_1_recall_cb": 0.5, "rouge_1_recall_ce": 0.5, "rouge_1_precision": 0.2449, "rouge_1_precision_cb": 0.2449, "rouge_1_precision_ce": 0.2449, "rouge_1_f_score": 0.32877, "rouge_1_f_score_cb": 0.32877, "rouge_1_f_score_ce": 0.32877, "rouge_2_recall": 0.26087, "rouge_2_recall_cb": 0.26087, "rouge_2_recall_ce": 0.26087, "rouge_2_precision": 0.125, "rouge_2_precision_cb": 0.125, "rouge_2_precision_ce": 0.125, "rouge_2_f_score": 0.16901, "rouge_2_f_score_cb": 0.16901, "rouge_2_f_score_ce": 0.16901, "rouge_3_recall": 0.13636, "rouge_3_recall_cb": 0.13636, "rouge_3_recall_ce": 0.13636, "rouge_3_precision": 0.06383, "rouge_3_precision_cb": 0.06383, "rouge_3_precision_ce": 0.06383, "rouge_3_f_score": 0.08696, "rouge_3_f_score_cb": 0.08696, "rouge_3_f_score_ce": 0.08696, "rouge_4_recall": 0.09524, "rouge_4_recall_cb": 0.09524, "rouge_4_recall_ce": 0.09524, "rouge_4_precision": 0.04348, "rouge_4_precision_cb": 0.04348, "rouge_4_precision_ce": 0.04348, "rouge_4_f_score": 0.0597, "rouge_4_f_score_cb": 0.0597, "rouge_4_f_score_ce": 0.0597, "rouge_l_recall": 0.45833, "rouge_l_recall_cb": 0.45833, "rouge_l_recall_ce": 0.45833, "rouge_l_precision": 0.22449, "rouge_l_precision_cb": 0.22449, "rouge_l_precision_ce": 0.22449, "rouge_l_f_score": 0.30137, "rouge_l_f_score_cb": 0.30137, "rouge_l_f_score_ce": 0.30137, "rouge_w_1.2_recall": 0.19298, "rouge_w_1.2_recall_cb": 0.19298, "rouge_w_1.2_recall_ce": 0.19298, "rouge_w_1.2_precision": 0.17847, "rouge_w_1.2_precision_cb": 0.17847, "rouge_w_1.2_precision_ce": 0.17847, "rouge_w_1.2_f_score": 0.18544, "rouge_w_1.2_f_score_cb": 0.18544, "rouge_w_1.2_f_score_ce": 0.18544, "rouge_s*_recall": 0.25, "rouge_s*_recall_cb": 0.25, "rouge_s*_recall_ce": 0.25, "rouge_s*_precision": 0.05867, "rouge_s*_precision_cb": 0.05867, "rouge_s*_precision_ce": 0.05867, "rouge_s*_f_score": 0.09504, "rouge_s*_f_score_cb": 0.09504, "rouge_s*_f_score_ce": 0.09504, "rouge_su*_recall": 0.26756, "rouge_su*_recall_cb": 0.26756, "rouge_su*_recall_ce": 0.26756, "rouge_su*_precision": 0.06536, "rouge_su*_precision_cb": 0.06536, "rouge_su*_precision_ce": 0.06536, "rouge_su*_f_score": 0.10506, "rouge_su*_f_score_cb": 0.10506, "rouge_su*_f_score_ce": 0.10506}, "bart_bleu": 9.362537959985184, "bart_meteor": 0.24098336475078302, "bert_sum_rouge": {"rouge_1_recall": 0.83333, "rouge_1_recall_cb": 0.83333, "rouge_1_recall_ce": 0.83333, "rouge_1_precision": 0.37736, "rouge_1_precision_cb": 0.37736, "rouge_1_precision_ce": 0.37736, "rouge_1_f_score": 0.51948, "rouge_1_f_score_cb": 0.51948, "rouge_1_f_score_ce": 0.51948, "rouge_2_recall": 0.6087, "rouge_2_recall_cb": 0.6087, "rouge_2_recall_ce": 0.6087, "rouge_2_precision": 0.26923, "rouge_2_precision_cb": 0.26923, "rouge_2_precision_ce": 0.26923, "rouge_2_f_score": 0.37333, "rouge_2_f_score_cb": 0.37333, "rouge_2_f_score_ce": 0.37333, "rouge_3_recall": 0.45455, "rouge_3_recall_cb": 0.45455, "rouge_3_recall_ce": 0.45455, "rouge_3_precision": 0.19608, "rouge_3_precision_cb": 0.19608, "rouge_3_precision_ce": 0.19608, "rouge_3_f_score": 0.27397, "rouge_3_f_score_cb": 0.27397, "rouge_3_f_score_ce": 0.27397, "rouge_4_recall": 0.42857, "rouge_4_recall_cb": 0.42857, "rouge_4_recall_ce": 0.42857, "rouge_4_precision": 0.18, "rouge_4_precision_cb": 0.18, "rouge_4_precision_ce": 0.18, "rouge_4_f_score": 0.25352, "rouge_4_f_score_cb": 0.25352, "rouge_4_f_score_ce": 0.25352, "rouge_l_recall": 0.83333, "rouge_l_recall_cb": 0.83333, "rouge_l_recall_ce": 0.83333, "rouge_l_precision": 0.37736, "rouge_l_precision_cb": 0.37736, "rouge_l_precision_ce": 0.37736, "rouge_l_f_score": 0.51948, "rouge_l_f_score_cb": 0.51948, "rouge_l_f_score_ce": 0.51948, "rouge_w_1.2_recall": 0.38987, "rouge_w_1.2_recall_cb": 0.38987, "rouge_w_1.2_recall_ce": 0.38987, "rouge_w_1.2_precision": 0.33334, "rouge_w_1.2_precision_cb": 0.33334, "rouge_w_1.2_precision_ce": 0.33334, "rouge_w_1.2_f_score": 0.3594, "rouge_w_1.2_f_score_cb": 0.3594, "rouge_w_1.2_f_score_ce": 0.3594, "rouge_s*_recall": 0.70652, "rouge_s*_recall_cb": 0.70652, "rouge_s*_recall_ce": 0.70652, "rouge_s*_precision": 0.14151, "rouge_s*_precision_cb": 0.14151, "rouge_s*_precision_ce": 0.14151, "rouge_s*_f_score": 0.23579, "rouge_s*_f_score_cb": 0.23579, "rouge_s*_f_score_ce": 0.23579, "rouge_su*_recall": 0.71572, "rouge_su*_recall_cb": 0.71572, "rouge_su*_recall_ce": 0.71572, "rouge_su*_precision": 0.14965, "rouge_su*_precision_cb": 0.14965, "rouge_su*_precision_ce": 0.14965, "rouge_su*_f_score": 0.24754, "rouge_su*_f_score_cb": 0.24754, "rouge_su*_f_score_ce": 0.24754}, "bert_sum_bleu": 26.682362538461717, "bert_sum_meteor": 0.3596021740627703, "bus_rouge": {"rouge_1_recall": 0.375, "rouge_1_recall_cb": 0.375, "rouge_1_recall_ce": 0.375, "rouge_1_precision": 0.33333, "rouge_1_precision_cb": 0.33333, "rouge_1_precision_ce": 0.33333, "rouge_1_f_score": 0.35294, "rouge_1_f_score_cb": 0.35294, "rouge_1_f_score_ce": 0.35294, "rouge_2_recall": 0.30435, "rouge_2_recall_cb": 0.30435, "rouge_2_recall_ce": 0.30435, "rouge_2_precision": 0.26923, "rouge_2_precision_cb": 0.26923, "rouge_2_precision_ce": 0.26923, "rouge_2_f_score": 0.28571, "rouge_2_f_score_cb": 0.28571, "rouge_2_f_score_ce": 0.28571, "rouge_3_recall": 0.22727, "rouge_3_recall_cb": 0.22727, "rouge_3_recall_ce": 0.22727, "rouge_3_precision": 0.2, "rouge_3_precision_cb": 0.2, "rouge_3_precision_ce": 0.2, "rouge_3_f_score": 0.21276, "rouge_3_f_score_cb": 0.21276, "rouge_3_f_score_ce": 0.21276, "rouge_4_recall": 0.14286, "rouge_4_recall_cb": 0.14286, "rouge_4_recall_ce": 0.14286, "rouge_4_precision": 0.125, "rouge_4_precision_cb": 0.125, "rouge_4_precision_ce": 0.125, "rouge_4_f_score": 0.13333, "rouge_4_f_score_cb": 0.13333, "rouge_4_f_score_ce": 0.13333, "rouge_l_recall": 0.33333, "rouge_l_recall_cb": 0.33333, "rouge_l_recall_ce": 0.33333, "rouge_l_precision": 0.2963, "rouge_l_precision_cb": 0.2963, "rouge_l_precision_ce": 0.2963, "rouge_l_f_score": 0.31373, "rouge_l_f_score_cb": 0.31373, "rouge_l_f_score_ce": 0.31373, "rouge_w_1.2_recall": 0.16133, "rouge_w_1.2_recall_cb": 0.16133, "rouge_w_1.2_recall_ce": 0.16133, "rouge_w_1.2_precision": 0.27077, "rouge_w_1.2_precision_cb": 0.27077, "rouge_w_1.2_precision_ce": 0.27077, "rouge_w_1.2_f_score": 0.20219, "rouge_w_1.2_f_score_cb": 0.20219, "rouge_w_1.2_f_score_ce": 0.20219, "rouge_s*_recall": 0.12319, "rouge_s*_recall_cb": 0.12319, "rouge_s*_recall_ce": 0.12319, "rouge_s*_precision": 0.09687, "rouge_s*_precision_cb": 0.09687, "rouge_s*_precision_ce": 0.09687, "rouge_s*_f_score": 0.10846, "rouge_s*_f_score_cb": 0.10846, "rouge_s*_f_score_ce": 0.10846, "rouge_su*_recall": 0.14047, "rouge_su*_recall_cb": 0.14047, "rouge_su*_recall_ce": 0.14047, "rouge_su*_precision": 0.11141, "rouge_su*_precision_cb": 0.11141, "rouge_su*_precision_ce": 0.11141, "rouge_su*_f_score": 0.12426, "rouge_su*_f_score_cb": 0.12426, "rouge_su*_f_score_ce": 0.12426}, "bus_bleu": 20.4708197395452, "bus_meteor": 0.16144925048132866, "pgn_rouge": {"rouge_1_recall": 0.5, "rouge_1_recall_cb": 0.5, "rouge_1_recall_ce": 0.5, "rouge_1_precision": 0.2, "rouge_1_precision_cb": 0.2, "rouge_1_precision_ce": 0.2, "rouge_1_f_score": 0.28571, "rouge_1_f_score_cb": 0.28571, "rouge_1_f_score_ce": 0.28571, "rouge_2_recall": 0.17391, "rouge_2_recall_cb": 0.17391, "rouge_2_recall_ce": 0.17391, "rouge_2_precision": 0.0678, "rouge_2_precision_cb": 0.0678, "rouge_2_precision_ce": 0.0678, "rouge_2_f_score": 0.09756, "rouge_2_f_score_cb": 0.09756, "rouge_2_f_score_ce": 0.09756, "rouge_3_recall": 0.0, "rouge_3_recall_cb": 0.0, "rouge_3_recall_ce": 0.0, "rouge_3_precision": 0.0, "rouge_3_precision_cb": 0.0, "rouge_3_precision_ce": 0.0, "rouge_3_f_score": 0.0, "rouge_3_f_score_cb": 0.0, "rouge_3_f_score_ce": 0.0, "rouge_4_recall": 0.0, "rouge_4_recall_cb": 0.0, "rouge_4_recall_ce": 0.0, "rouge_4_precision": 0.0, "rouge_4_precision_cb": 0.0, "rouge_4_precision_ce": 0.0, "rouge_4_f_score": 0.0, "rouge_4_f_score_cb": 0.0, "rouge_4_f_score_ce": 0.0, "rouge_l_recall": 0.41667, "rouge_l_recall_cb": 0.41667, "rouge_l_recall_ce": 0.41667, "rouge_l_precision": 0.16667, "rouge_l_precision_cb": 0.16667, "rouge_l_precision_ce": 0.16667, "rouge_l_f_score": 0.2381, "rouge_l_f_score_cb": 0.2381, "rouge_l_f_score_ce": 0.2381, "rouge_w_1.2_recall": 0.17346, "rouge_w_1.2_recall_cb": 0.17346, "rouge_w_1.2_recall_ce": 0.17346, "rouge_w_1.2_precision": 0.13101, "rouge_w_1.2_precision_cb": 0.13101, "rouge_w_1.2_precision_ce": 0.13101, "rouge_w_1.2_f_score": 0.14928, "rouge_w_1.2_f_score_cb": 0.14928, "rouge_w_1.2_f_score_ce": 0.14928, "rouge_s*_recall": 0.23188, "rouge_s*_recall_cb": 0.23188, "rouge_s*_recall_ce": 0.23188, "rouge_s*_precision": 0.03616, "rouge_s*_precision_cb": 0.03616, "rouge_s*_precision_ce": 0.03616, "rouge_s*_f_score": 0.06256, "rouge_s*_f_score_cb": 0.06256, "rouge_s*_f_score_ce": 0.06256, "rouge_su*_recall": 0.25084, "rouge_su*_recall_cb": 0.25084, "rouge_su*_recall_ce": 0.25084, "rouge_su*_precision": 0.04101, "rouge_su*_precision_cb": 0.04101, "rouge_su*_precision_ce": 0.04101, "rouge_su*_f_score": 0.07049, "rouge_su*_f_score_cb": 0.07049, "rouge_su*_f_score_ce": 0.07049}, "pgn_bleu": 2.5660839596410767, "pgn_meteor": 0.20857037317951116, "s2s_rouge": {"rouge_1_recall": 0.04167, "rouge_1_recall_cb": 0.04167, "rouge_1_recall_ce": 0.04167, "rouge_1_precision": 0.03125, "rouge_1_precision_cb": 0.03125, "rouge_1_precision_ce": 0.03125, "rouge_1_f_score": 0.03572, "rouge_1_f_score_cb": 0.03572, "rouge_1_f_score_ce": 0.03572, "rouge_2_recall": 0.0, "rouge_2_recall_cb": 0.0, "rouge_2_recall_ce": 0.0, "rouge_2_precision": 0.0, "rouge_2_precision_cb": 0.0, "rouge_2_precision_ce": 0.0, "rouge_2_f_score": 0.0, "rouge_2_f_score_cb": 0.0, "rouge_2_f_score_ce": 0.0, "rouge_3_recall": 0.0, "rouge_3_recall_cb": 0.0, "rouge_3_recall_ce": 0.0, "rouge_3_precision": 0.0, "rouge_3_precision_cb": 0.0, "rouge_3_precision_ce": 0.0, "rouge_3_f_score": 0.0, "rouge_3_f_score_cb": 0.0, "rouge_3_f_score_ce": 0.0, "rouge_4_recall": 0.0, "rouge_4_recall_cb": 0.0, "rouge_4_recall_ce": 0.0, "rouge_4_precision": 0.0, "rouge_4_precision_cb": 0.0, "rouge_4_precision_ce": 0.0, "rouge_4_f_score": 0.0, "rouge_4_f_score_cb": 0.0, "rouge_4_f_score_ce": 0.0, "rouge_l_recall": 0.04167, "rouge_l_recall_cb": 0.04167, "rouge_l_recall_ce": 0.04167, "rouge_l_precision": 0.03125, "rouge_l_precision_cb": 0.03125, "rouge_l_precision_ce": 0.03125, "rouge_l_f_score": 0.03572, "rouge_l_f_score_cb": 0.03572, "rouge_l_f_score_ce": 0.03572, "rouge_w_1.2_recall": 0.02207, "rouge_w_1.2_recall_cb": 0.02207, "rouge_w_1.2_recall_ce": 0.02207, "rouge_w_1.2_precision": 0.03125, "rouge_w_1.2_precision_cb": 0.03125, "rouge_w_1.2_precision_ce": 0.03125, "rouge_w_1.2_f_score": 0.02587, "rouge_w_1.2_f_score_cb": 0.02587, "rouge_w_1.2_f_score_ce": 0.02587, "rouge_s*_recall": 0.0, "rouge_s*_recall_cb": 0.0, "rouge_s*_recall_ce": 0.0, "rouge_s*_precision": 0.0, "rouge_s*_precision_cb": 0.0, "rouge_s*_precision_ce": 0.0, "rouge_s*_f_score": 0.0, "rouge_s*_f_score_cb": 0.0, "rouge_s*_f_score_ce": 0.0, "rouge_su*_recall": 0.00334, "rouge_su*_recall_cb": 0.00334, "rouge_su*_recall_ce": 0.00334, "rouge_su*_precision": 0.0019, "rouge_su*_precision_cb": 0.0019, "rouge_su*_precision_ce": 0.0019, "rouge_su*_f_score": 0.00242, "rouge_su*_f_score_cb": 0.00242, "rouge_su*_f_score_ce": 0.00242}, "s2s_bleu": 1.389736936231864, "s2s_meteor": 0.04468744753469528}