{"BERTS2S": "the ministry of defence will close its barracks in brecon, the ministry of defence has said.", "BERTS2S_lines": ["the ministry of defence will close its barracks in brecon, the ministry of defence has said."], "TConvS2S": "the ministry of defence has announced it will close its barracks in brecon, south wales, as part of a nationwide closure.", "TConvS2S_lines": ["the ministry of defence has announced it will close its barracks in brecon, south wales, as part of a nationwide closure."], "Gold": "the ministry of defence is to close the british army\\'s welsh headquarters at brecon, powys, in 2027.", "Gold_lines": ["the ministry of defence is to close the british army\\'s welsh headquarters at brecon, powys, in 2027."], "PtGen": "a decision has been made to close a well-loved and historic historic barracks in pembrokeshire, the defence secretary has said.", "PtGen_lines": ["a decision has been made to close a well-loved and historic historic barracks in pembrokeshire, the defence secretary has said."], "TranS2S": "lord brecon will step down as defence secretary, the defence secretary has announced.", "TranS2S_lines": ["lord brecon will step down as defence secretary, the defence secretary has announced."], "article": "It comes in a shake-up of UK military buildings and resettling of regiments.Brecon and Radnorshire Conservative MP Chris Davies condemned the closure, saying there had been a barracks in Brecon since 1805, home to troops who fought the Zulus at Rorke's Drift.\"This decision is abhorrent and I shall be fighting it every step of the way,\" he said.\"The government has a great deal of questions to answer over why it is proposing to close a well-loved and historic barracks in a vitally important military town.\"Brecon Barracks has served our country with distinction over its long history, with soldiers from the site fighting in every conflict since the early 19th century.\"This decision shows a blatant lack of respect for that history.\"Mr Davies said he was launching a petition against the decision, saying the Brecon area had some of the highest unemployment levels in Wales.He also hoped the closure would not damage the town's \"thriving\" military tourism industry.Brecon barracks has about 85 civilian staff and 90 military but it is not thought jobs are at risk.Mr Davies said he understood the nearby Sennybridge training ground and infantry school at Dering Lines would not be affected.Defence Secretary Sir Michael Fallon told the Commons on Monday the reorganisation in Wales would see a specialist light infantry centre created at St Athan, Vale of Glamorgan.Cawdor Barracks, Pembrokeshire - whose closure was previously announced in 2013 - will now shut in 2024, while a storage depot at Sennybridge will go in 2025.Responding for Labour, Shadow Defence Secretary Nia Griffith, MP for Llanelli, said the ministry was \"right to restructure its estate\".But she warned closing bases would affect the livelihoods of many people who would face \"gnawing uncertainty\" over their future.", "article_lines": ["It comes in a shake-up of UK military buildings and resettling of regiments.", "Brecon and Radnorshire Conservative MP Chris Davies condemned the closure, saying there had been a barracks in Brecon since 1805, home to troops who fought the Zulus at Rorke's Drift.", "\"This decision is abhorrent and I shall be fighting it every step of the way,\" he said.", "\"The government has a great deal of questions to answer over why it is proposing to close a well-loved and historic barracks in a vitally important military town.", "\"Brecon", "Barracks has served our country with distinction over its long history, with soldiers from the site fighting in every conflict since the early 19th century.", "\"This decision shows a blatant lack of respect for that history.", "\"Mr", "Davies said he was launching a petition against the decision, saying the Brecon area had some of the highest unemployment levels in Wales.", "He also hoped the closure would not damage the town's \"thriving\" military tourism industry.", "Brecon barracks has about 85 civilian staff and 90 military", "but it is not thought jobs are at risk.", "Mr Davies said he understood the nearby Sennybridge training ground and infantry school at Dering Lines would not be affected.", "Defence Secretary Sir Michael Fallon told the Commons on Monday the reorganisation in Wales would see a specialist light infantry centre created at St Athan, Vale of Glamorgan.", "Cawdor Barracks, Pembrokeshire - whose closure was previously announced in 2013 - will now shut in 2024, while a storage depot at Sennybridge will go in 2025.Responding for Labour, Shadow Defence Secretary Nia Griffith, MP for Llanelli, said the ministry was \"right to restructure its estate\".", "But she warned closing bases would affect the livelihoods of many people who would face \"gnawing uncertainty\" over their future."], "entity_counter": {"a shake-up": 1, "UK military buildings": 1, "resettling": 1, "regiments": 1, "Davies": 3, "the closure": 2, "a barracks": 1, "Brecon": 1, "troops": 1, "the Zulus": 1, "Rorke's Drift": 1, "\"This decision": 3, "the way": 1, "\"The government": 1, "a great deal": 1, "questions": 1, "a well-loved and historic barracks": 1, "a vitally important military town": 1, "\"Brecon": 1, "Barracks": 1, "our country": 1, "distinction": 1, "its long history": 1, "soldiers": 1, "the site": 1, "every conflict": 1, "a blatant lack": 1, "respect": 1, "that history": 1, "a petition": 1, "the Brecon area": 1, "the highest unemployment levels": 1, "Wales": 2, "the town's \"thriving\" military tourism industry": 1, "a barracks in Brecon": 1, "about 85 civilian staff": 1, "90 military": 1, "jobs": 1, "risk": 1, "the nearby Sennybridge training ground and infantry school": 1, "Dering Lines": 1, "Defence Secretary Sir Michael Fallon": 1, "the Commons": 1, "the reorganisation": 1, "a specialist light infantry centre": 1, "St Athan": 1, "Vale": 1, "Glamorgan": 1, "Cawdor Barracks": 1, "Pembrokeshire": 1, "whose closure": 1, "a storage depot": 1, "Sennybridge": 1, "Labour": 1, "Shadow Defence Secretary Nia Griffith": 1, "MP": 1, "Llanelli": 1, "the ministry": 1, "its estate\"": 1, "closing bases": 1, "the livelihoods": 1, "many people": 1, "uncertainty": 1, "their future": 1}, "negative_entity": "judge", "url": "http://web.archive.org/web/20170421020723/http://www.bbc.co.uk/news/uk-wales-politics-37902151", "hash": "37902151", "traps": [["F4", "the site at knottingley, west yorkshire is to burn fuel from refuse, industrial and commercial waste including wood, said operator multifuel energy ltd."], ["F4", "share this withemailfacebookmessengermessengertwitterpinterestwhatsapplinkedincopy this linkthe victim was fatally injured outside a property on daniel hill terrace, upperthorpe, close to the city centre."], ["F4", "aim is to cover eventual interpretations that could be given to the contracts drawn up in the operation to sign neymar,\" a club statement read."], ["F1", "but it is not thought jobs are not at risk ."], ["F0", "it comes in a shake-up of uk military buildings and resettling of regiments."], ["F4", "against the euro, sterling was down 0.6% at \u00e2\u201a\u00ac1.2605 and weakened by 1% against the japanese yen to just over 151.investors have been spooked by data showing the chances of a remain vote have fallen, although markets have also been rattled by global economic worries."]], "model_names": ["BERTS2S_lines", "TConvS2S_lines", "Gold_lines", "PtGen_lines", "TranS2S_lines"], "BERTS2S_rouge": {"rouge_1_recall": 0.44444, "rouge_1_recall_cb": 0.44444, "rouge_1_recall_ce": 0.44444, "rouge_1_precision": 0.5, "rouge_1_precision_cb": 0.5, "rouge_1_precision_ce": 0.5, "rouge_1_f_score": 0.47059, "rouge_1_f_score_cb": 0.47059, "rouge_1_f_score_ce": 0.47059, "rouge_2_recall": 0.17647, "rouge_2_recall_cb": 0.17647, "rouge_2_recall_ce": 0.17647, "rouge_2_precision": 0.2, "rouge_2_precision_cb": 0.2, "rouge_2_precision_ce": 0.2, "rouge_2_f_score": 0.1875, "rouge_2_f_score_cb": 0.1875, "rouge_2_f_score_ce": 0.1875, "rouge_3_recall": 0.125, "rouge_3_recall_cb": 0.125, "rouge_3_recall_ce": 0.125, "rouge_3_precision": 0.14286, "rouge_3_precision_cb": 0.14286, "rouge_3_precision_ce": 0.14286, "rouge_3_f_score": 0.13333, "rouge_3_f_score_cb": 0.13333, "rouge_3_f_score_ce": 0.13333, "rouge_4_recall": 0.06667, "rouge_4_recall_cb": 0.06667, "rouge_4_recall_ce": 0.06667, "rouge_4_precision": 0.07692, "rouge_4_precision_cb": 0.07692, "rouge_4_precision_ce": 0.07692, "rouge_4_f_score": 0.07143, "rouge_4_f_score_cb": 0.07143, "rouge_4_f_score_ce": 0.07143, "rouge_l_recall": 0.33333, "rouge_l_recall_cb": 0.33333, "rouge_l_recall_ce": 0.33333, "rouge_l_precision": 0.375, "rouge_l_precision_cb": 0.375, "rouge_l_precision_ce": 0.375, "rouge_l_f_score": 0.35294, "rouge_l_f_score_cb": 0.35294, "rouge_l_f_score_ce": 0.35294, "rouge_w_1.2_recall": 0.16847, "rouge_w_1.2_recall_cb": 0.16847, "rouge_w_1.2_recall_ce": 0.16847, "rouge_w_1.2_precision": 0.33785, "rouge_w_1.2_precision_cb": 0.33785, "rouge_w_1.2_precision_ce": 0.33785, "rouge_w_1.2_f_score": 0.22483, "rouge_w_1.2_f_score_cb": 0.22483, "rouge_w_1.2_f_score_ce": 0.22483, "rouge_s*_recall": 0.1634, "rouge_s*_recall_cb": 0.1634, "rouge_s*_recall_ce": 0.1634, "rouge_s*_precision": 0.20833, "rouge_s*_precision_cb": 0.20833, "rouge_s*_precision_ce": 0.20833, "rouge_s*_f_score": 0.18315, "rouge_s*_f_score_cb": 0.18315, "rouge_s*_f_score_ce": 0.18315, "rouge_su*_recall": 0.19412, "rouge_su*_recall_cb": 0.19412, "rouge_su*_recall_ce": 0.19412, "rouge_su*_precision": 0.24444, "rouge_su*_precision_cb": 0.24444, "rouge_su*_precision_ce": 0.24444, "rouge_su*_f_score": 0.21639, "rouge_su*_f_score_cb": 0.21639, "rouge_su*_f_score_ce": 0.21639}, "BERTS2S_bleu": 14.54720117650069, "BERTS2S_meteor": 0.20241827996637654, "TConvS2S_rouge": {"rouge_1_recall": 0.38889, "rouge_1_recall_cb": 0.38889, "rouge_1_recall_ce": 0.38889, "rouge_1_precision": 0.33333, "rouge_1_precision_cb": 0.33333, "rouge_1_precision_ce": 0.33333, "rouge_1_f_score": 0.35897, "rouge_1_f_score_cb": 0.35897, "rouge_1_f_score_ce": 0.35897, "rouge_2_recall": 0.17647, "rouge_2_recall_cb": 0.17647, "rouge_2_recall_ce": 0.17647, "rouge_2_precision": 0.15, "rouge_2_precision_cb": 0.15, "rouge_2_precision_ce": 0.15, "rouge_2_f_score": 0.16216, "rouge_2_f_score_cb": 0.16216, "rouge_2_f_score_ce": 0.16216, "rouge_3_recall": 0.125, "rouge_3_recall_cb": 0.125, "rouge_3_recall_ce": 0.125, "rouge_3_precision": 0.10526, "rouge_3_precision_cb": 0.10526, "rouge_3_precision_ce": 0.10526, "rouge_3_f_score": 0.11428, "rouge_3_f_score_cb": 0.11428, "rouge_3_f_score_ce": 0.11428, "rouge_4_recall": 0.06667, "rouge_4_recall_cb": 0.06667, "rouge_4_recall_ce": 0.06667, "rouge_4_precision": 0.05556, "rouge_4_precision_cb": 0.05556, "rouge_4_precision_ce": 0.05556, "rouge_4_f_score": 0.06061, "rouge_4_f_score_cb": 0.06061, "rouge_4_f_score_ce": 0.06061, "rouge_l_recall": 0.33333, "rouge_l_recall_cb": 0.33333, "rouge_l_recall_ce": 0.33333, "rouge_l_precision": 0.28571, "rouge_l_precision_cb": 0.28571, "rouge_l_precision_ce": 0.28571, "rouge_l_f_score": 0.30769, "rouge_l_f_score_cb": 0.30769, "rouge_l_f_score_ce": 0.30769, "rouge_w_1.2_recall": 0.16294, "rouge_w_1.2_recall_cb": 0.16294, "rouge_w_1.2_recall_ce": 0.16294, "rouge_w_1.2_precision": 0.24896, "rouge_w_1.2_precision_cb": 0.24896, "rouge_w_1.2_precision_ce": 0.24896, "rouge_w_1.2_f_score": 0.19697, "rouge_w_1.2_f_score_cb": 0.19697, "rouge_w_1.2_f_score_ce": 0.19697, "rouge_s*_recall": 0.13072, "rouge_s*_recall_cb": 0.13072, "rouge_s*_recall_ce": 0.13072, "rouge_s*_precision": 0.09524, "rouge_s*_precision_cb": 0.09524, "rouge_s*_precision_ce": 0.09524, "rouge_s*_f_score": 0.11019, "rouge_s*_f_score_cb": 0.11019, "rouge_s*_f_score_ce": 0.11019, "rouge_su*_recall": 0.15882, "rouge_su*_recall_cb": 0.15882, "rouge_su*_recall_ce": 0.15882, "rouge_su*_precision": 0.11739, "rouge_su*_precision_cb": 0.11739, "rouge_su*_precision_ce": 0.11739, "rouge_su*_f_score": 0.135, "rouge_su*_f_score_cb": 0.135, "rouge_su*_f_score_ce": 0.135}, "TConvS2S_bleu": 13.308442527111016, "TConvS2S_meteor": 0.22301741734189318, "PtGen_rouge": {"rouge_1_recall": 0.27778, "rouge_1_recall_cb": 0.27778, "rouge_1_recall_ce": 0.27778, "rouge_1_precision": 0.2381, "rouge_1_precision_cb": 0.2381, "rouge_1_precision_ce": 0.2381, "rouge_1_f_score": 0.25641, "rouge_1_f_score_cb": 0.25641, "rouge_1_f_score_ce": 0.25641, "rouge_2_recall": 0.05882, "rouge_2_recall_cb": 0.05882, "rouge_2_recall_ce": 0.05882, "rouge_2_precision": 0.05, "rouge_2_precision_cb": 0.05, "rouge_2_precision_ce": 0.05, "rouge_2_f_score": 0.05405, "rouge_2_f_score_cb": 0.05405, "rouge_2_f_score_ce": 0.05405, "rouge_3_recall": 0.0, "rouge_3_recall_cb": 0.0, "rouge_3_recall_ce": 0.0, "rouge_3_precision": 0.0, "rouge_3_precision_cb": 0.0, "rouge_3_precision_ce": 0.0, "rouge_3_f_score": 0.0, "rouge_3_f_score_cb": 0.0, "rouge_3_f_score_ce": 0.0, "rouge_4_recall": 0.0, "rouge_4_recall_cb": 0.0, "rouge_4_recall_ce": 0.0, "rouge_4_precision": 0.0, "rouge_4_precision_cb": 0.0, "rouge_4_precision_ce": 0.0, "rouge_4_f_score": 0.0, "rouge_4_f_score_cb": 0.0, "rouge_4_f_score_ce": 0.0, "rouge_l_recall": 0.16667, "rouge_l_recall_cb": 0.16667, "rouge_l_recall_ce": 0.16667, "rouge_l_precision": 0.14286, "rouge_l_precision_cb": 0.14286, "rouge_l_precision_ce": 0.14286, "rouge_l_f_score": 0.15385, "rouge_l_f_score_cb": 0.15385, "rouge_l_f_score_ce": 0.15385, "rouge_w_1.2_recall": 0.0935, "rouge_w_1.2_recall_cb": 0.0935, "rouge_w_1.2_recall_ce": 0.0935, "rouge_w_1.2_precision": 0.14286, "rouge_w_1.2_precision_cb": 0.14286, "rouge_w_1.2_precision_ce": 0.14286, "rouge_w_1.2_f_score": 0.11303, "rouge_w_1.2_f_score_cb": 0.11303, "rouge_w_1.2_f_score_ce": 0.11303, "rouge_s*_recall": 0.03922, "rouge_s*_recall_cb": 0.03922, "rouge_s*_recall_ce": 0.03922, "rouge_s*_precision": 0.02857, "rouge_s*_precision_cb": 0.02857, "rouge_s*_precision_ce": 0.02857, "rouge_s*_f_score": 0.03306, "rouge_s*_f_score_cb": 0.03306, "rouge_s*_f_score_ce": 0.03306, "rouge_su*_recall": 0.06471, "rouge_su*_recall_cb": 0.06471, "rouge_su*_recall_ce": 0.06471, "rouge_su*_precision": 0.04783, "rouge_su*_precision_cb": 0.04783, "rouge_su*_precision_ce": 0.04783, "rouge_su*_f_score": 0.055, "rouge_su*_f_score_cb": 0.055, "rouge_su*_f_score_ce": 0.055}, "PtGen_bleu": 4.724932626401583, "PtGen_meteor": 0.11361448023134205, "TranS2S_rouge": {"rouge_1_recall": 0.16667, "rouge_1_recall_cb": 0.16667, "rouge_1_recall_ce": 0.16667, "rouge_1_precision": 0.23077, "rouge_1_precision_cb": 0.23077, "rouge_1_precision_ce": 0.23077, "rouge_1_f_score": 0.19355, "rouge_1_f_score_cb": 0.19355, "rouge_1_f_score_ce": 0.19355, "rouge_2_recall": 0.0, "rouge_2_recall_cb": 0.0, "rouge_2_recall_ce": 0.0, "rouge_2_precision": 0.0, "rouge_2_precision_cb": 0.0, "rouge_2_precision_ce": 0.0, "rouge_2_f_score": 0.0, "rouge_2_f_score_cb": 0.0, "rouge_2_f_score_ce": 0.0, "rouge_3_recall": 0.0, "rouge_3_recall_cb": 0.0, "rouge_3_recall_ce": 0.0, "rouge_3_precision": 0.0, "rouge_3_precision_cb": 0.0, "rouge_3_precision_ce": 0.0, "rouge_3_f_score": 0.0, "rouge_3_f_score_cb": 0.0, "rouge_3_f_score_ce": 0.0, "rouge_4_recall": 0.0, "rouge_4_recall_cb": 0.0, "rouge_4_recall_ce": 0.0, "rouge_4_precision": 0.0, "rouge_4_precision_cb": 0.0, "rouge_4_precision_ce": 0.0, "rouge_4_f_score": 0.0, "rouge_4_f_score_cb": 0.0, "rouge_4_f_score_ce": 0.0, "rouge_l_recall": 0.11111, "rouge_l_recall_cb": 0.11111, "rouge_l_recall_ce": 0.11111, "rouge_l_precision": 0.15385, "rouge_l_precision_cb": 0.15385, "rouge_l_precision_ce": 0.15385, "rouge_l_f_score": 0.12903, "rouge_l_f_score_cb": 0.12903, "rouge_l_f_score_ce": 0.12903, "rouge_w_1.2_recall": 0.05553, "rouge_w_1.2_recall_cb": 0.05553, "rouge_w_1.2_recall_ce": 0.05553, "rouge_w_1.2_precision": 0.13706, "rouge_w_1.2_precision_cb": 0.13706, "rouge_w_1.2_precision_ce": 0.13706, "rouge_w_1.2_f_score": 0.07904, "rouge_w_1.2_f_score_cb": 0.07904, "rouge_w_1.2_f_score_ce": 0.07904, "rouge_s*_recall": 0.01307, "rouge_s*_recall_cb": 0.01307, "rouge_s*_recall_ce": 0.01307, "rouge_s*_precision": 0.02564, "rouge_s*_precision_cb": 0.02564, "rouge_s*_precision_ce": 0.02564, "rouge_s*_f_score": 0.01731, "rouge_s*_f_score_cb": 0.01731, "rouge_s*_f_score_ce": 0.01731, "rouge_su*_recall": 0.02941, "rouge_su*_recall_cb": 0.02941, "rouge_su*_recall_ce": 0.02941, "rouge_su*_precision": 0.05556, "rouge_su*_precision_cb": 0.05556, "rouge_su*_precision_ce": 0.05556, "rouge_su*_f_score": 0.03846, "rouge_su*_f_score_cb": 0.03846, "rouge_su*_f_score_ce": 0.03846}, "TranS2S_bleu": 2.4642841067740786, "TranS2S_meteor": 0.10331842427296434}